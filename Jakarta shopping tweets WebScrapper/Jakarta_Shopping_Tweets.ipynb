{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jakarta Shopping_Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a25YJKBuAyCQ",
        "outputId": "f169a579-25b1-4166-8d58-a7212418d641"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\r\u001b[K     |▍                               | 10kB 13.3MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 16.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 12.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61kB 5.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 102kB 5.4MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 5.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122kB 5.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 143kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 153kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 174kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 184kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 194kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 204kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 215kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 225kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 235kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 245kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 256kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 266kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 276kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 286kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 296kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 307kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 317kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 327kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 337kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 348kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 358kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 368kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 378kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 389kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 399kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 409kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 419kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 430kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 440kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 450kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 460kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 471kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 481kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 491kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 501kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 512kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 522kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 532kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 542kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 552kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 563kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 573kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 583kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 593kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 604kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 614kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 624kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 634kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 645kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 655kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 665kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 675kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 686kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 696kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 706kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 716kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 727kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 737kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 747kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 757kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 768kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 778kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 788kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 798kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 808kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 819kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 829kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 839kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 849kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 860kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 870kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 880kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 890kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 901kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 911kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,418 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,221 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,780 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,188 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [911 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,658 kB]\n",
            "Fetched 11.4 MB in 3s (3,380 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 86.0 MB of archives.\n",
            "After this operation, 298 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 91.0.4472.101-0ubuntu0.18.04.1 [1,124 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 91.0.4472.101-0ubuntu0.18.04.1 [76.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 91.0.4472.101-0ubuntu0.18.04.1 [3,937 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 91.0.4472.101-0ubuntu0.18.04.1 [4,837 kB]\n",
            "Fetched 86.0 MB in 5s (19.1 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_91.0.4472.101-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_91.0.4472.101-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_91.0.4472.101-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_91.0.4472.101-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA1oSHhaAPlN"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9F2X5ipBKW3"
      },
      "source": [
        "#keyword scraping approach, combined use of selenium and beautiful soup\n",
        "\n",
        "#tweet_url function will return correct url to target\n",
        "\n",
        "#notes on expected format of inputs, strings\n",
        "#date format: YYYY-MM-DD\n",
        "#loc: City\n",
        "#radius: 150mi or 100mi\n",
        "#function will read keyword as hashtag automatically (in respect of Twitter robots.txt file)\n",
        "\n",
        "def tweet_url(keyword, loc, beg_date, end_date, radius):\n",
        "    targeting = 'https://twitter.com/search?q=' + keyword + '%20near%3A' + loc + '%20since%3A' + beg_date + '%20until%3A' + end_date + '%20within%3A' + radius + '&src=recent_search_click&f=live'\n",
        "    return targeting"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkp87PxBB5yQ"
      },
      "source": [
        "#trying to get all html content of twitter search page\n",
        "\n",
        "#manual crawl function tries to collect all page info as you scroll down\n",
        "#manual because we set number of scrolls\n",
        "#limited by number of scrolls set, might not be end of page yet if set too low, say, below 100\n",
        "\n",
        "def manual_crawl_page(url, n):\n",
        "    #loading search page\n",
        "    driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "    driver.get(url)\n",
        "    driver.implicitly_wait(15)\n",
        "    \n",
        "    #empty list where we will append all html content collected as we scroll down\n",
        "    html=[]\n",
        "    \n",
        "    #scroll for n seconds\n",
        "    for i in range(n):\n",
        "        #search for all relevant contents of twitter search page\n",
        "        list_items = driver.find_elements_by_class_name('css-1dbjc4n')\n",
        "        #collecting html content\n",
        "        html.append(list_items[0].get_attribute('innerHTML'))\n",
        "        \n",
        "        #to scroll down\n",
        "        elem = driver.find_element_by_tag_name('body')\n",
        "        elem.send_keys(Keys.END)\n",
        "        \n",
        "        time.sleep(np.random.randint(1,3))\n",
        "    \n",
        "    #parsing each html content collected\n",
        "    soup = []\n",
        "    for i in html:    \n",
        "        soup += BeautifulSoup(i,'lxml')\n",
        "        \n",
        "    return soup"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwL2MhlXB79p"
      },
      "source": [
        "#auto crawl function tries to collect all page info as it automatically scrolls down until end of page\n",
        "#note: takes some time to finish, also extracts fewer tweets. might be due to pace of scrolling\n",
        "\n",
        "def auto_crawl_page(url):\n",
        "    #loading search page\n",
        "    driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "    driver.get(url)    \n",
        "    \n",
        "    time.sleep(np.random.randint(1,3))  \n",
        "    \n",
        "    #empty list where we will append all html content collected as we scroll down\n",
        "    html = []\n",
        "    \n",
        "    #calculates length of page\n",
        "    lenpage = driver.execute_script(\"var lenpage=document.body.scrollHeight;return lenpage;\")      \n",
        "    \n",
        "    #to iterate until bottom of page\n",
        "    match = False\n",
        "    while(match==False):\n",
        "        #implicitly wait to load page\n",
        "        driver.implicitly_wait(10)\n",
        "        \n",
        "        #search for all relevant contents of twitter search page\n",
        "        list_items = driver.find_elements_by_class_name('css-1dbjc4n')\n",
        "        #collecting html content\n",
        "        html.append(list_items[0].get_attribute('innerHTML'))\n",
        "        \n",
        "        #re-assigns value of length of page\n",
        "        lastcount = lenpage\n",
        "        \n",
        "        time.sleep(np.random.randint(10,30))\n",
        "        \n",
        "        #to scroll down\n",
        "        elem = driver.find_element_by_tag_name('body')\n",
        "        elem.send_keys(Keys.END)\n",
        "        \n",
        "        #re-calculates length of page\n",
        "        lenpage = driver.execute_script(\"var lenpage=document.body.scrollHeight;return lenpage;\")\n",
        "        \n",
        "        #criteria that evaluates if we are at the bottom of page already\n",
        "        if lastcount==lenpage:\n",
        "            match=True\n",
        "    \n",
        "    #parsing each html content collected\n",
        "    soup = []\n",
        "    for i in html:    \n",
        "        soup += BeautifulSoup(i,'lxml')\n",
        "        \n",
        "    return soup"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB-WnPmHCCks"
      },
      "source": [
        "#function extracting tweet details\n",
        "\n",
        "def tweetsrc_data(soup):\n",
        "    #get all tweets from each parsed html\n",
        "    tweets = []\n",
        "    for a in soup:\n",
        "        temp = a.find_all('article')\n",
        "        for b in temp:\n",
        "            tweets.append(b)\n",
        "    \n",
        "    #empty lists where we will append tweet info\n",
        "    post = []\n",
        "    timestamp = []\n",
        "    retweet = []\n",
        "    like = []\n",
        "    reply = []\n",
        "\n",
        "    #to iterate over tweets collected, if-else is to account for unexpected errors\n",
        "    for i in tweets:\n",
        "        #post\n",
        "        post.append(re.sub('(.*)\\@(\\w+)\\W\\d+h?\\s?(Mar|Apr|May)?','',i.get_text())) #post\n",
        "        \n",
        "        #timestamp\n",
        "        if i.find('time')==None:\n",
        "            timestamp.append(np.nan)\n",
        "        elif len(i.find('time')['datetime'])==19:\n",
        "            timestamp.append(i.find('time')['datetime'])\n",
        "        elif len(i.find('time')['datetime'])==24:\n",
        "            timestamp.append(i.find('time')['datetime'].replace('.000Z',''))\n",
        "        \n",
        "        #retweets\n",
        "        if i.find('div',{'data-testid':'retweet'})==None:\n",
        "            retweet.append(0)\n",
        "        else:\n",
        "            retweet.append(i.find('div',{'data-testid':'retweet'}).get_text()) #retweet\n",
        "        \n",
        "        #likes\n",
        "        if i.find('div',{'data-testid':'like'})==None:\n",
        "            like.append(0)\n",
        "        else:\n",
        "            like.append(i.find('div',{'data-testid':'like'}).get_text())#like\n",
        "        \n",
        "        #replies\n",
        "        if i.find('div',{'data-testid':'reply'})==None:\n",
        "            reply.append(0)\n",
        "        else:\n",
        "            reply.append(i.find('div',{'data-testid':'reply'}).get_text()) #reply\n",
        "    \n",
        "    #making dataframe of all tweet info\n",
        "    df = pd.DataFrame({'timestamp':timestamp, 'post':post, 'reply':reply, 'like':like, 'retweet':retweet})\n",
        "    \n",
        "    #some tweets don't have necessary tag for tweet stats so it returns blanks, replacing them with 0\n",
        "    df['reply'] = df['reply'].replace('',0)\n",
        "    df['like'] = df['like'].replace('',0)\n",
        "    df['retweet'] = df['retweet'].replace('',0)\n",
        "    \n",
        "    #drop duplicate tweets which may be collected repeatedly due to overlaps in page info while scrolling\n",
        "    df = df.drop_duplicates(subset=['timestamp', 'post', 'reply', 'like', 'retweet'], keep='first')\n",
        "    \n",
        "    #making sure indices are in order\n",
        "    df = df.reset_index()\n",
        "    df = df.drop(columns='index')\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfAbgwNwCO0o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "outputId": "f47807aa-d52f-486d-ba08-bc6f78ca5154"
      },
      "source": [
        "#sample data\n",
        "sample_url = tweet_url('shopping', 'jakarta', '2021-07-17', '2021-07-18', '150mi')\n",
        "sample_soup = manual_crawl_page(sample_url, 50)\n",
        "\n",
        "tweetsrc_data(sample_soup)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>post</th>\n",
              "      <th>reply</th>\n",
              "      <th>like</th>\n",
              "      <th>retweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-07-17T21:08:17</td>\n",
              "      <td>WANITA\\nMenangis bukan karena lemah\\nTapi kare...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-07-17T20:49:29</td>\n",
              "      <td>Replying to @radenraufLawakan suami. Karna kal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-07-17T19:11:11</td>\n",
              "      <td>kira kira malem2 gini menjelang pagi masih ada...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-07-17T19:09:59</td>\n",
              "      <td>Dicari Reseller Masker Wajah Alami di Wonosari...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-07-17T18:59:21</td>\n",
              "      <td>Sandal Casual Pria Hexpreme #Sandal #sandalPri...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2021-07-17T18:44:46</td>\n",
              "      <td>shopping w my mom is fun she be like “yo te lo...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2021-07-17T18:26:03</td>\n",
              "      <td>Answer These Shopping Math Questions To Find O...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2021-07-17T18:20:58</td>\n",
              "      <td>Replying to @tawan_vihokrdulu window shopping ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2021-07-17T18:16:03</td>\n",
              "      <td>AnisaChiBi teteh anchib...boleh minta follback...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2021-07-17T17:38:43</td>\n",
              "      <td>#ETC See if i can HODL my futures ETCUSDT, hop...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2021-07-17T17:09:24</td>\n",
              "      <td>Hallo tweps Dk_Onlineshop menjual aneka ragam ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021-07-17T16:31:37</td>\n",
              "      <td>Selesai gue transaksi, gue nunggu obat sambil ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2021-07-17T15:33:56</td>\n",
              "      <td>makanan fav, shopping, nanaQuote Tweetclose.@s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2021-07-17T15:08:56</td>\n",
              "      <td>People already mentioned Netflix party as a wa...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2021-07-17T14:13:01</td>\n",
              "      <td>Replying to @FWBESSBelanja bareng aja nder sek...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2021-07-17T13:41:38</td>\n",
              "      <td>https://lynk.id/informapondokindah…\\n*CASHBACK...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2021-07-17T08:11:58</td>\n",
              "      <td>ibunya James@dinivaldiani·Jul 17Hehehehe ayooo...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2021-07-17T07:57:35</td>\n",
              "      <td>Ayuta@yutaberry·Jul 17Barusan grocery shopping...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2021-07-17T07:00:51</td>\n",
              "      <td>Sukma@Sukma532·Jul 17Happy shopping tweeps ^^</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2021-07-17T06:40:21</td>\n",
              "      <td>Riri Dahye @Mochacyno·Jul 17Replying to @shopp...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2021-07-17T05:52:32</td>\n",
              "      <td>feli @feli_umeshu·Jul 17me: i feel like i need...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2021-07-17T04:38:34</td>\n",
              "      <td>vivo_indonesia@vivo_indonesia·Jul 17Yuk join I...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2021-07-17T04:00:45</td>\n",
              "      <td>ONLINE SHOPPING@delvin_collect·Jul 17IklanTerU...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2021-07-17T03:35:00</td>\n",
              "      <td>M-News@mnews_id·Jul 17Kemenkop UKM dan Shopee ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2021-07-17T03:30:41</td>\n",
              "      <td>ONLINE SHOPPING@delvin_collect·Jul 17Chek Foto...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2021-07-17T02:57:53</td>\n",
              "      <td>Khalid@khalids·Jul 17Replying to @mhfzhshm @fa...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2021-07-17T02:09:33</td>\n",
              "      <td>Dk Onlineshop@Dk_Onlineshop·Jul 17Males kemana...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2021-07-17T00:30:07</td>\n",
              "      <td>Dimas Padmanegara@dimashadi1103·Jul 17Replying...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2021-07-17T00:20:51</td>\n",
              "      <td>Katerine Wen@katerine_wen14·Jul 17Hari ini mas...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              timestamp  ... retweet\n",
              "0   2021-07-17T21:08:17  ...       0\n",
              "1   2021-07-17T20:49:29  ...       0\n",
              "2   2021-07-17T19:11:11  ...       0\n",
              "3   2021-07-17T19:09:59  ...       0\n",
              "4   2021-07-17T18:59:21  ...       0\n",
              "5   2021-07-17T18:44:46  ...       0\n",
              "6   2021-07-17T18:26:03  ...       0\n",
              "7   2021-07-17T18:20:58  ...       1\n",
              "8   2021-07-17T18:16:03  ...       0\n",
              "9   2021-07-17T17:38:43  ...       0\n",
              "10  2021-07-17T17:09:24  ...       0\n",
              "11  2021-07-17T16:31:37  ...       0\n",
              "12  2021-07-17T15:33:56  ...       0\n",
              "13  2021-07-17T15:08:56  ...       0\n",
              "14  2021-07-17T14:13:01  ...       0\n",
              "15  2021-07-17T13:41:38  ...       0\n",
              "16  2021-07-17T08:11:58  ...       0\n",
              "17  2021-07-17T07:57:35  ...       0\n",
              "18  2021-07-17T07:00:51  ...       0\n",
              "19  2021-07-17T06:40:21  ...       0\n",
              "20  2021-07-17T05:52:32  ...       0\n",
              "21  2021-07-17T04:38:34  ...       1\n",
              "22  2021-07-17T04:00:45  ...       0\n",
              "23  2021-07-17T03:35:00  ...       0\n",
              "24  2021-07-17T03:30:41  ...       0\n",
              "25  2021-07-17T02:57:53  ...       0\n",
              "26  2021-07-17T02:09:33  ...       0\n",
              "27  2021-07-17T00:30:07  ...       0\n",
              "28  2021-07-17T00:20:51  ...       0\n",
              "\n",
              "[29 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcfGa6Ek5QFH"
      },
      "source": [
        "#full data in 2021\n",
        "url = tweet_url('shopping', 'Jakarta', '2021-01-01', '2021-07-18', '150mi')\n",
        "soup = manual_crawl_page(url, 1000)\n",
        "data = tweetsrc_data(soup)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "WFSIp6UnTIBV",
        "outputId": "fc2ebbd0-f11b-4009-ad9d-2b1f8f159bf8"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>post</th>\n",
              "      <th>reply</th>\n",
              "      <th>like</th>\n",
              "      <th>retweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-07-17T21:08:17</td>\n",
              "      <td>WANITA\\nMenangis bukan karena lemah\\nTapi kare...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-07-17T18:26:03</td>\n",
              "      <td>Answer These Shopping Math Questions To Find O...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-07-17T18:20:58</td>\n",
              "      <td>Replying to @tawan_vihokrdulu window shopping ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-07-17T17:38:43</td>\n",
              "      <td>#ETC See if i can HODL my futures ETCUSDT, hop...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-07-17T17:09:24</td>\n",
              "      <td>Hallo tweps Dk_Onlineshop menjual aneka ragam ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             timestamp  ... retweet\n",
              "0  2021-07-17T21:08:17  ...       0\n",
              "1  2021-07-17T18:26:03  ...       0\n",
              "2  2021-07-17T18:20:58  ...       1\n",
              "3  2021-07-17T17:38:43  ...       0\n",
              "4  2021-07-17T17:09:24  ...       0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEB87EuAH5E5",
        "outputId": "0e6ac265-553b-4a24-e5d6-72a560adb474"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(403, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CQbtnNBuVnN"
      },
      "source": [
        "#remove irrelevant records\n",
        "data = data[~data['timestamp'].isnull()]\n",
        "\n",
        "#prettify indices\n",
        "data = data.reset_index()\n",
        "data = data.drop(columns='index')\n",
        "\n",
        "#fixing timestamps and making new date and time columns\n",
        "data['timestamp'] = data['timestamp'].apply(str)\n",
        "data['date'] = pd.to_datetime(data['timestamp'].apply(lambda x: x.split('T')[0])).dt.date\n",
        "data['time'] = pd.to_datetime(data['timestamp'].apply(lambda x: x[11:])).dt.time\n",
        "data['timestamp'] = data['date'].apply(str) + \" \" + data['time'].apply(str)\n",
        "data['timestamp'] = pd.to_datetime(data['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "#cleaning post content\n",
        "data['post'] = data['post'].str.replace('\\n',' ')\n",
        "\n",
        "#to fix likes and retweets, removes K-- which indicates thousand values\n",
        "def kLikes(row):\n",
        "    if row['like'] == None:\n",
        "        row['like'] = '0'\n",
        "    if 'K' in row['like']:\n",
        "        digits = re.findall('\\d+', row['like'])\n",
        "        if len(digits) == 1:\n",
        "            return digits[0] + '000'\n",
        "        elif len(digits) == 2:\n",
        "            return digits[0] + digits[1] +'00'\n",
        "    else:\n",
        "        return row['like']\n",
        "\n",
        "def kRetweets(row):\n",
        "    if row['retweet'] == None:\n",
        "        row['retweet'] = '0'\n",
        "    if 'K' in row['retweet']:\n",
        "        digits = re.findall('\\d+', row['retweet'])\n",
        "        if len(digits) == 1:\n",
        "            return digits[0] + '000'\n",
        "        elif len(digits) == 2:\n",
        "            return digits[0] + digits[1] +'00'\n",
        "    else:\n",
        "        return row['retweet']\n",
        "\n",
        "data['like'] = data['like'].astype('str')\n",
        "data['retweet'] = data['retweet'].astype('str')\n",
        "data['like'] =data.apply(kLikes, axis = 1)\n",
        "data['retweet'] = data.apply(kRetweets, axis = 1)\n",
        "data['like'] = data['like'].astype('int64')\n",
        "data['retweet'] = data['retweet'].astype('int64')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkmqAuBeut9T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b75ba5af-c2c7-43db-814a-82680516769f"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>post</th>\n",
              "      <th>reply</th>\n",
              "      <th>like</th>\n",
              "      <th>retweet</th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-07-17 21:08:17</td>\n",
              "      <td>WANITA Menangis bukan karena lemah Tapi karena...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-07-17</td>\n",
              "      <td>21:08:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-07-17 18:26:03</td>\n",
              "      <td>Answer These Shopping Math Questions To Find O...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-07-17</td>\n",
              "      <td>18:26:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-07-17 18:20:58</td>\n",
              "      <td>Replying to @tawan_vihokrdulu window shopping ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-07-17</td>\n",
              "      <td>18:20:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-07-17 17:38:43</td>\n",
              "      <td>#ETC See if i can HODL my futures ETCUSDT, hop...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-07-17</td>\n",
              "      <td>17:38:43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-07-17 17:09:24</td>\n",
              "      <td>Hallo tweps Dk_Onlineshop menjual aneka ragam ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-07-17</td>\n",
              "      <td>17:09:24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            timestamp  ...      time\n",
              "0 2021-07-17 21:08:17  ...  21:08:17\n",
              "1 2021-07-17 18:26:03  ...  18:26:03\n",
              "2 2021-07-17 18:20:58  ...  18:20:58\n",
              "3 2021-07-17 17:38:43  ...  17:38:43\n",
              "4 2021-07-17 17:09:24  ...  17:09:24\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEF8C159K-gr",
        "outputId": "65f0f0dd-a1c0-4ff8-a30d-3be3d136d8dd"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(402, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWQy2acVD6SD"
      },
      "source": [
        "#write data to CSV file\n",
        "data.to_csv(\"shopping_tweet.csv\", index = False, header = True)"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}