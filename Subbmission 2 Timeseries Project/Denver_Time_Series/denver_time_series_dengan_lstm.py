# -*- coding: utf-8 -*-
"""Denver Time Series dengan LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ugHeaAZlqor_OlQMAkxhRW7DLoB8hVpV
"""

# Menginstal package kaggle
!pip install -q kaggle

from google.colab import files

# Mengupload file json dari profile kaggle
files.upload()

# Membuat direktory dan mengubah izin file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d kadenswarr/hourly-weather-data-in-denver

# Ekstrak file zip dan melihat isi dataset
!mkdir hourly-weather-data-in-denver
!unzip hourly-weather-data-in-denver.zip -d hourly-weather-data-in-denver
!ls hourly-weather-data-in-denver

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

data_train = pd.read_csv('hourly-weather-data-in-denver/weather_data_long.csv')
data_train.head()

data_train.shape

# Mengecek apakah ada data yang null
data_train.isnull().sum()

# Memilih kolom yang akan digunakan
df = data_train.iloc[100000:145233][['Date', 'Temperature']]

dates = df['Date'].values
temp = df['Temperature'].astype('float').values

# Membuat plot data
plt.figure(figsize=(15,5))
plt.plot(dates, temp)
plt.title('Temperature average', fontsize=20)

from sklearn.model_selection import train_test_split

# Membagi dataset menjadi train dan test
X_train, X_test, Y_train, Y_test = train_test_split(temp, dates, test_size=0.2, shuffle=False)

print(len(X_train), len(X_test))

min_mae = (temp.max() - temp.min()) * 0.1
print(min_mae)

# merubah data menjadi format yang dapat diterima oleh model
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(X_train, window_size=32, batch_size=64, shuffle_buffer=1000)
val_set = windowed_dataset(X_test, window_size=32, batch_size=64, shuffle_buffer=1000)

model = tf.keras.models.Sequential([                          
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64),
  tf.keras.layers.Dense(32, activation="relu"),
  tf.keras.layers.Dense(16, activation="relu"),
  tf.keras.layers.Dense(1),  
])

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=['mae'])

# Membuat custom callbacks
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae') < min_mae):
      print('\nMAE has reach < 10% from total data!')
      self.model.stop_training = True

cust_callbacks = myCallback()

from keras.callbacks import ReduceLROnPlateau

hist = model.fit(train_set, validation_data=val_set, 
                 epochs=100, callbacks=[cust_callbacks, ReduceLROnPlateau()])

plt.plot(hist.history['mae'])
plt.plot(hist.history['val_mae'])
plt.title('Mean Absolut Error Model')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Loss Model')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'test'], loc='upper left')
plt.show()