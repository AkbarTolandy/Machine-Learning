# -*- coding: utf-8 -*-
"""Subbmission 1 project NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19FizTlN2z95D2jX5CZMVLYq8lOp5vf74

Akbar
Submission 1 NLP
"""

# Menginstal package kaggle
!pip install -q kaggle

from google.colab import files

# Mengupload file json dari profile kaggle
files.upload()

# Membuat direktory dan mengubah izin file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# Mendownload dataset
!kaggle datasets download -d ritresearch/happydb

# Ekstrak file zip dan melihat isi dataset
!mkdir happydb
!unzip happydb.zip -d happydb
!ls happydb

import pandas as pd

# Mengubah dataset menjadi dataframe
df = pd.read_csv('happydb/cleaned_hm.csv')

df.head()

# Melihat isi total dari data
df.shape

# Mengecek nilai null
df.isnull().sum()

# Melihat isi data klasifikasi kategori
df.predicted_category.value_counts()

# Menggabungkan data class bonding, leisure, nature, exercise ke dalam enjoy_the_moment
for i in range(0,len(df)):
  if(df['predicted_category'][i]=='bonding'):
        df['predicted_category'][i]='enjoy_the_moment'
  elif(df['predicted_category'][i]=='leisure'):
        df['predicted_category'][i]='enjoy_the_moment'
  elif(df['predicted_category'][i]=='nature'):
        df['predicted_category'][i]='enjoy_the_moment'
  elif(df['predicted_category'][i]=='exercise'):
        df['predicted_category'][i]='enjoy_the_moment'

df.predicted_category.value_counts()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
# %config InlineBackend.figure_format = 'retina'

# visualisasi data class dari kategori
sns.countplot(df.predicted_category)
plt.show()

# Menghapus kolom yang tidak digunakan
df = df.drop(columns=['hmid',	'wid',	'reflection_period',
                      'original_hm', 'modified',	
                      'num_sentence', 'ground_truth_category'])

# Melakukan proses one hot encoding pada kategori
category = pd.get_dummies(df.predicted_category)
new_df = pd.concat([df, category], axis=1)
new_df = new_df.drop(columns='predicted_category')
new_df

# Menghapus kolom yang tidak di perlukan
hm = new_df['cleaned_hm'].values
sentiment = new_df[[
                 'achievement', 
                 'affection',
                 'enjoy_the_moment']].values
df

# Mengecek isi text dari cleaned_hm
hm

from sklearn.model_selection import train_test_split

# Membagi dataset menjadi train dan test
X_train, X_test, Y_train, Y_test = train_test_split(hm, category, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

num_word=10000

tokenizer = Tokenizer(num_words=num_word, lower=True, oov_token='x')
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(sequences_train, maxlen=200, truncating='post')
padded_test = pad_sequences(sequences_test, maxlen=200, truncating='post')

import tensorflow as tf

# Membuat arsitektur CNN dengan Keras
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=num_word, output_dim=16),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(3, activation='softmax')
])

# Compile model menggunakan adam
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

from keras.callbacks import LearningRateScheduler, EarlyStopping

# Mengurangi kecepatan pelatihan per 5 epoch
def scheduler(epoch, lr):
  if epoch != 0 and epoch % 5 == 0:
    return lr * 0.2
  else:
    return lr

reduce_lr = LearningRateScheduler(scheduler, verbose=1)

# Menghentikan proses pelatihan apabila tidak terjadi perubahan setelah 3 epoch
early_stop = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)

# Membuat custom callbacks
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      print('\naccuracy and val_accuracy > 90%!')
      self.model.stop_training = True

cust_callbacks = myCallback()

callbacks_list = [reduce_lr, early_stop, cust_callbacks]

hist = model.fit(padded_train, Y_train, epochs=100, callbacks=callbacks_list,
                 steps_per_epoch=32, validation_data=(padded_test, Y_test), verbose=2)

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Accuracy Model')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Loss Model')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'test'], loc='upper left')
plt.show()